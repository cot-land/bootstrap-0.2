// ARM64 Code Generator
// Generates ARM64 machine code from SSA representation.
//
// Reference: src/codegen/arm64.zig (our Zig bootstrap)
// Reference: ~/learning/go/src/cmd/compile/internal/arm64/ssa.go
//
// Design:
// 1. Walk SSA blocks in scheduled order
// 2. For each value, emit corresponding ARM64 instructions
// 3. Handle register allocation results
// 4. Emit prologue/epilogue for function frame

import "../arm64/asm.cot"
import "../arm64/regs.cot"

// ============================================================================
// Code Emitter
// ============================================================================

// The emitter collects machine code instructions.
// For self-hosting, we use a simple byte buffer approach.
// Maximum code size for a single function (can be increased if needed).
const MAX_CODE_SIZE: i64 = 262144;

struct Emitter {
    // Code buffer (would be dynamically allocated in full implementation)
    code_offset: i64,
    // Number of instructions emitted
    inst_count: i64,
}

fn Emitter_init() Emitter {
    return Emitter{
        .code_offset = 0,
        .inst_count = 0,
    };
}

// ============================================================================
// Type-Based Instruction Selection
// ============================================================================
// Following Go's pattern from arm64/ssa.go

// Select load instruction based on type size
fn Instruction_selectLoad(size: i64, is_signed: bool) i64 {
    // Returns an instruction selector code
    // 1 = LDRB, 2 = LDRSB, 3 = LDRH, 4 = LDRSH, 5 = LDR (32), 6 = LDRSW, 7 = LDR (64)
    if size == 1 {
        if is_signed { return 2; }  // LDRSB
        return 1;  // LDRB
    }
    if size == 2 {
        if is_signed { return 4; }  // LDRSH
        return 3;  // LDRH
    }
    if size == 4 {
        if is_signed { return 6; }  // LDRSW
        return 5;  // LDR W
    }
    return 7;  // LDR X (64-bit)
}

// Select store instruction based on type size
fn Instruction_selectStore(size: i64) i64 {
    // Returns an instruction selector code
    // 1 = STRB, 2 = STRH, 3 = STR (32), 4 = STR (64)
    if size == 1 { return 1; }  // STRB
    if size == 2 { return 2; }  // STRH
    if size == 4 { return 3; }  // STR W
    return 4;  // STR X (64-bit)
}

// ============================================================================
// Instruction Encoding Helpers
// ============================================================================

// Encode a load instruction based on selector and operands
fn Instruction_encodeLoad(selector: i64, rd: i64, rn: i64, offset: i64) i64 {
    if selector == 1 { return encode_ldrb(rd, rn, offset); }
    // TODO: Add LDRSB, LDRH, LDRSH, etc. as needed
    if selector == 7 { return encode_ldr(rd, rn, offset); }
    // Default to 64-bit load
    return encode_ldr(rd, rn, offset);
}

// Encode a store instruction based on selector and operands
fn Instruction_encodeStore(selector: i64, rt: i64, rn: i64, offset: i64) i64 {
    if selector == 1 { return encode_strb(rt, rn, offset); }
    // TODO: Add STRH, STR W as needed
    if selector == 4 { return encode_str(rt, rn, offset); }
    // Default to 64-bit store
    return encode_str(rt, rn, offset);
}

// ============================================================================
// Frame Layout
// ============================================================================

// ARM64 calling convention:
// - X0-X7: Arguments and return values
// - X8: Indirect result location
// - X9-X15: Temporary registers (caller-saved)
// - X16-X17: Intra-procedure-call scratch (used by linker)
// - X18: Platform register (reserved)
// - X19-X28: Callee-saved registers
// - X29 (FP): Frame pointer
// - X30 (LR): Link register
// - SP: Stack pointer (must be 16-byte aligned)

// Standard function prologue:
// STP X29, X30, [SP, #-frame_size]!  ; Push FP and LR, adjust SP
// MOV X29, SP                         ; Set up frame pointer
// (save callee-saved registers if needed)

// Standard function epilogue:
// (restore callee-saved registers if needed)
// LDP X29, X30, [SP], #frame_size    ; Restore FP and LR, adjust SP
// RET

// Encode function prologue
// Returns array of instruction words (simplified - returns first instruction)
fn ARM64_encodePrologue(frame_size: i64) i64 {
    // STP X29, X30, [SP, #-frame_size]!
    // For now, use simple SUB SP, SP, #frame_size approach
    // then separate stores
    if frame_size > 0 {
        // SUB SP, SP, #frame_size
        return encode_sub_imm(SP, SP, frame_size);
    }
    // NOP if no frame
    return encode_nop();
}

// Encode function epilogue (return instruction)
fn ARM64_encodeEpilogue(frame_size: i64) i64 {
    // For now, just encode RET
    // Full implementation would restore SP first
    return encode_ret(LR);
}

// Note: STP/LDP functions are in arm64/asm.cot:
// encode_stp_pre(rt1, rt2, rn, imm7) - pre-indexed store pair
// encode_ldp_post(rt1, rt2, rn, imm7) - post-indexed load pair
// imm7 is the offset in 8-byte units (so -16 bytes = imm7 of -2)

// ============================================================================
// Move Generation
// ============================================================================

// Generate a move from one register to another
fn ARM64_encodeMovReg(rd: i64, rm: i64) i64 {
    // MOV Xd, Xm is actually ORR Xd, XZR, Xm
    return encode_orr_reg(rd, XZR, rm);
}

// Generate loading an immediate value
// For values that fit in 16 bits, use MOVZ
// For larger values, need MOVZ + MOVK sequence
fn ARM64_encodeMovImm(rd: i64, imm: i64) i64 {
    // Simple case: 16-bit immediate
    if imm >= 0 and imm <= 65535 {
        return encode_movz(rd, imm, 0);
    }
    // For larger immediates, we'd need multiple instructions
    // For now, just encode the low 16 bits
    return encode_movz(rd, imm & 65535, 0);
}

// ============================================================================
// Comparison and Condition Codes
// ============================================================================

// After a comparison, these condition codes are used:
// EQ (0): Equal (Z=1)
// NE (1): Not equal (Z=0)
// LT (11): Signed less than (N!=V)
// GT (12): Signed greater than (Z=0 and N=V)
// LE (13): Signed less or equal (Z=1 or N!=V)
// GE (10): Signed greater or equal (N=V)

fn Cond_forSignedLt() i64 { return COND_LT; }
fn Cond_forSignedGt() i64 { return COND_GT; }
fn Cond_forSignedLe() i64 { return COND_LE; }
fn Cond_forSignedGe() i64 { return COND_GE; }
fn Cond_forEq() i64 { return COND_EQ; }
fn Cond_forNe() i64 { return COND_NE; }

// ============================================================================
// SSA Operation Handlers
// ============================================================================

// These functions take SSA operation parameters and return encoded instructions.
// In the full implementation, these would be called from the main codegen loop.

// Handle SSA add operation
fn ARM64_add(rd: i64, rn: i64, rm: i64) i64 {
    return encode_add_reg(rd, rn, rm);
}

// Handle SSA add immediate operation
fn ARM64_addConst(rd: i64, rn: i64, imm: i64) i64 {
    if imm >= 0 and imm <= 4095 {
        return encode_add_imm(rd, rn, imm);
    }
    // For larger immediates, need to load into temp register first
    // Simplified: just use the low 12 bits
    return encode_add_imm(rd, rn, imm & 4095);
}

// Handle SSA sub operation
fn ARM64_sub(rd: i64, rn: i64, rm: i64) i64 {
    return encode_sub_reg(rd, rn, rm);
}

// Handle SSA sub immediate operation
fn ARM64_subConst(rd: i64, rn: i64, imm: i64) i64 {
    if imm >= 0 and imm <= 4095 {
        return encode_sub_imm(rd, rn, imm);
    }
    return encode_sub_imm(rd, rn, imm & 4095);
}

// Handle SSA and operation
fn ARM64_and(rd: i64, rn: i64, rm: i64) i64 {
    return encode_and_reg(rd, rn, rm);
}

// Handle SSA or operation
fn ARM64_or(rd: i64, rn: i64, rm: i64) i64 {
    return encode_orr_reg(rd, rn, rm);
}

// Handle SSA xor operation
fn ARM64_xor(rd: i64, rn: i64, rm: i64) i64 {
    return encode_eor_reg(rd, rn, rm);
}

// Handle SSA comparison (sets flags)
fn ARM64_cmp(rn: i64, rm: i64) i64 {
    return encode_cmp_reg(rn, rm);
}

// Handle conditional set (result = cond ? 1 : 0)
fn ARM64_setcc(rd: i64, cond: i64) i64 {
    return encode_cset(rd, cond);
}

// Handle conditional select
fn ARM64_select(rd: i64, rn: i64, rm: i64, cond: i64) i64 {
    return encode_csel(rd, rn, rm, cond);
}

// ============================================================================
// Branch Generation
// ============================================================================

// Generate unconditional branch
fn ARM64_branch(offset_words: i64) i64 {
    return encode_b(offset_words);
}

// Generate conditional branch (branch if condition is true)
fn ARM64_branchCond(offset_words: i64, cond: i64) i64 {
    return encode_b_cond(offset_words, cond);
}

// Generate function call
fn ARM64_call(offset_words: i64) i64 {
    return encode_bl(offset_words);
}

// Generate return
fn ARM64_return() i64 {
    return encode_ret(LR);
}

// ============================================================================
// Memory Access
// ============================================================================

// Load from memory (64-bit)
fn ARM64_load64(rd: i64, base: i64, offset_8bytes: i64) i64 {
    return encode_ldr(rd, base, offset_8bytes);
}

// Store to memory (64-bit)
fn ARM64_store64(rt: i64, base: i64, offset_8bytes: i64) i64 {
    return encode_str(rt, base, offset_8bytes);
}

// Load byte from memory
fn ARM64_load8(rd: i64, base: i64, offset_bytes: i64) i64 {
    return encode_ldrb(rd, base, offset_bytes);
}

// Store byte to memory
fn ARM64_store8(rt: i64, base: i64, offset_bytes: i64) i64 {
    return encode_strb(rt, base, offset_bytes);
}
