// Cot0 SSA Builder - IR to SSA Conversion
// Converts frontend IR to backend SSA form.
//
// Key concepts (from Go):
// - Convert IR nodes to SSA values
// - Track variable→value mapping per block
// - Handle control flow (if, while, return)
// - Create phi nodes at merge points
//
// Reference: ~/learning/go/src/cmd/compile/internal/ssagen/ssa.go
// Reference: bootstrap-0.2/src/frontend/ssa_builder.zig

// Import func.cot for Block, Value, Local types
// The Zig compiler deduplicates imports, so this won't cause duplicate definitions
import "func.cot"

// ============================================================================
// Constants
// ============================================================================

const MAX_VAR_DEFS: i64 = 1024;    // Max variable definitions per block
const MAX_BLOCK_DEFS: i64 = 500;   // Max blocks with definitions

// Initial capacities for SSABuilder-owned storage (matches bootstrap Zig pattern)
const SB_INIT_BLOCK_DEFS: i64 = 500;
const SB_INIT_VAR_DEFS: i64 = 50000;
const SB_INIT_BLOCK_MAP: i64 = 500;
// BUG FIX: Was 100000, but with 400+ blocks * 1024 vars = 400k+ needed
// When var_storage is reallocated, all BlockDefs.values pointers become dangling!
// Increase to 1M to avoid realloc during compilation of large functions.
// TODO: Proper fix is to store indices instead of pointers, or update all
// BlockDefs.values pointers after realloc.
const SB_INIT_VAR_STORAGE: i64 = 1000000;
const SB_INIT_FWD_VARS: i64 = 1000;

// Generic sized allocation - use @sizeOf(T) to compute struct sizes at compile time
extern fn malloc_sized(count: i64, struct_size: i64) *u8;
extern fn realloc_sized(ptr: *u8, old_count: i64, new_count: i64, struct_size: i64) *u8;
extern fn malloc_i64(count: i64) *i64;
extern fn realloc_i64(ptr: *i64, old_count: i64, new_count: i64) *i64;

// ============================================================================
// Module-Level Storage (self-allocated)
// Following Zig pattern: modules own their storage, allocated on first use
// ============================================================================

var sb_all_defs: *BlockDefs = null;
var sb_all_defs_cap: i64 = 0;
var sb_block_map: *BlockMapping = null;
var sb_block_map_cap: i64 = 0;
// OPTIMIZED: node_values is now direct-indexed *i64 array for O(1) lookup
// Index = ir_idx - ir_nodes_start, Value = SSA value_id (or INVALID_ID)
var sb_node_values: *i64 = null;
var sb_node_values_cap: i64 = 0;
var sb_var_storage: *i64 = null;  // Direct-indexed arrays for BlockDefs (values[var_idx] = value_id)
var sb_var_storage_cap: i64 = 0;
var sb_fwd_vars: *VarDef = null;
var sb_fwd_vars_cap: i64 = 0;
// OPTIMIZATION: Sparse indices for O(1) lookup (instead of O(n) linear scan)
var sb_block_map_index: *i64 = null;   // index[ir_block] = position in block_map (or -1)
var sb_block_map_index_cap: i64 = 0;
var sb_fwd_vars_index: *i64 = null;    // index[var_idx] = value_id (or INVALID_ID)
var sb_fwd_vars_index_cap: i64 = 0;
var sb_storage_initialized: bool = false;

// Allocate module storage if not already allocated
fn SSABuilder_allocateStorage() {
    if sb_storage_initialized {
        return;
    }

    // BlockDefs allocation - use @sizeOf for correct struct size
    // All entries are cleared (uninitialized) for O(1) direct indexing by block_id
    sb_all_defs_cap = SB_INIT_BLOCK_DEFS;
    let blockdefs_size: i64 = @sizeOf(BlockDefs);
    let blockdefs_raw: *u8 = malloc_sized(sb_all_defs_cap, blockdefs_size);
    sb_all_defs = @ptrCast(*BlockDefs, blockdefs_raw);

    // Initialize all entries as uninitialized for direct indexing
    var bd_idx: i64 = 0;
    while bd_idx < sb_all_defs_cap {
        let bd: *BlockDefs = sb_all_defs + bd_idx;
        BlockDefs_clear(bd);
        bd_idx = bd_idx + 1;
    }

    // BlockMapping allocation - use @sizeOf for correct struct size
    sb_block_map_cap = SB_INIT_BLOCK_MAP;
    let blockmap_size: i64 = @sizeOf(BlockMapping);
    let blockmap_raw: *u8 = malloc_sized(sb_block_map_cap, blockmap_size);
    sb_block_map = @ptrCast(*BlockMapping, blockmap_raw);

    // VarDef allocations - use @sizeOf for correct struct size
    let vardef_size: i64 = @sizeOf(VarDef);

    // OPTIMIZED: node_values is direct-indexed i64 array (not VarDef)
    sb_node_values_cap = SB_INIT_VAR_DEFS;
    sb_node_values = malloc_i64(sb_node_values_cap);

    sb_var_storage_cap = SB_INIT_VAR_STORAGE;
    sb_var_storage = malloc_i64(sb_var_storage_cap);  // Direct-indexed i64 arrays for BlockDefs

    sb_fwd_vars_cap = SB_INIT_FWD_VARS;
    let fwdvars_raw: *u8 = malloc_sized(sb_fwd_vars_cap, vardef_size);
    sb_fwd_vars = @ptrCast(*VarDef, fwdvars_raw);

    // OPTIMIZATION: Allocate sparse indices for O(1) lookup
    // block_map_index: ir_block -> position in block_map
    sb_block_map_index_cap = SB_INIT_BLOCK_MAP;
    sb_block_map_index = malloc_i64(sb_block_map_index_cap);
    var bmi: i64 = 0;
    while bmi < sb_block_map_index_cap {
        (sb_block_map_index + bmi).* = -1;  // -1 means not in block_map
        bmi = bmi + 1;
    }

    // fwd_vars_index: var_idx -> value_id (direct values, not positions)
    sb_fwd_vars_index_cap = SB_INIT_FWD_VARS;
    sb_fwd_vars_index = malloc_i64(sb_fwd_vars_index_cap);
    var fvi: i64 = 0;
    while fvi < sb_fwd_vars_index_cap {
        (sb_fwd_vars_index + fvi).* = INVALID_ID;  // INVALID_ID means no cached value
        fvi = fvi + 1;
    }

    sb_storage_initialized = true;
}

// IR node indices (from lower.cot/ir.cot)
const IR_NULL_NODE: i64 = -1;

// ============================================================================
// IR to SSA Op Mapping (from main.cot)
// Reference: Zig's binaryOpToSSA() in ssa_builder.zig
// ============================================================================

// Map IR binary operation to SSA Op
fn Op_fromIRBinaryOp(ir_op: i64) Op {
    // IRBinaryOp enum values (from ir.cot)
    if ir_op == 0 { return Op.Add64; }     // IRBinaryOp.Add
    if ir_op == 1 { return Op.Sub64; }     // IRBinaryOp.Sub
    if ir_op == 2 { return Op.Mul64; }     // IRBinaryOp.Mul
    if ir_op == 3 { return Op.Div64; }     // IRBinaryOp.Div
    if ir_op == 4 { return Op.Mod64; }     // IRBinaryOp.Mod
    if ir_op == 5 { return Op.Eq64; }      // IRBinaryOp.Eq
    if ir_op == 6 { return Op.Ne64; }      // IRBinaryOp.Ne
    if ir_op == 7 { return Op.Lt64; }      // IRBinaryOp.Lt
    if ir_op == 8 { return Op.Le64; }      // IRBinaryOp.Le
    if ir_op == 9 { return Op.Gt64; }      // IRBinaryOp.Gt
    if ir_op == 10 { return Op.Ge64; }     // IRBinaryOp.Ge
    if ir_op == 11 { return Op.And64; }    // IRBinaryOp.And (logical - use bitwise)
    if ir_op == 12 { return Op.Or64; }     // IRBinaryOp.Or (logical - use bitwise)
    if ir_op == 13 { return Op.And64; }    // IRBinaryOp.BitAnd
    if ir_op == 14 { return Op.Or64; }     // IRBinaryOp.BitOr
    if ir_op == 15 { return Op.Xor64; }    // IRBinaryOp.BitXor
    if ir_op == 16 { return Op.Shl64; }    // IRBinaryOp.Shl
    if ir_op == 17 { return Op.Shr64; }    // IRBinaryOp.Shr
    return Op.Add64;  // Default fallback
}

fn Op_fromIRUnaryOp(ir_op: i64) Op {
    // IR unary op constants from lower.cot: NEG=18, NOT=19, BIT_NOT=20
    if ir_op == 18 { return Op.Neg64; }        // IR_OP_NEG
    if ir_op == 19 { return Op.LogicalNot64; } // IR_OP_NOT (logical: !x)
    if ir_op == 20 { return Op.Not64; }        // IR_OP_BIT_NOT (bitwise: ~x)
    return Op.Neg64;  // Default fallback
}

// ============================================================================
// Variable Definition Tracking
// ============================================================================

// Single variable → value mapping
struct VarDef {
    var_idx: i64,         // Local variable index
    value_id: i64,        // SSA value ID
}

// Variable definitions for one block
// OPTIMIZED: Uses direct-indexed array for O(1) lookup (like Zig's HashMap)
// Array is indexed by var_idx, stores value_id (-1 = not defined)
struct BlockDefs {
    block_id: i64,
    values: *i64,         // Direct-indexed array: values[var_idx] = value_id
    values_cap: i64,      // Capacity (max var_idx + 1)
    defs_count: i64,      // Number of defined variables (for stats/iteration)
    initialized: bool,    // True if this block has been visited
}

fn BlockDefs_init(bd: *BlockDefs, block_id: i64, values: *i64, cap: i64) {
    bd.block_id = block_id;
    bd.values = values;
    bd.values_cap = cap;
    bd.defs_count = 0;
    bd.initialized = true;
    // Initialize all slots to INVALID_ID (-1)
    var i: i64 = 0;
    while i < cap {
        (values + i).* = INVALID_ID;
        i = i + 1;
    }
}

// Mark BlockDefs as uninitialized (used when pre-allocating the array)
fn BlockDefs_clear(bd: *BlockDefs) {
    bd.block_id = -1;
    bd.values = null;
    bd.values_cap = 0;
    bd.defs_count = 0;
    bd.initialized = false;
}

// Set variable value - O(1) direct array access
fn BlockDefs_set(bd: *BlockDefs, var_idx: i64, value_id: i64) {
    if var_idx < 0 or var_idx >= bd.values_cap { return; }
    let old_val: i64 = (bd.values + var_idx).*;
    (bd.values + var_idx).* = value_id;
    // Track count for stats
    if old_val == INVALID_ID and value_id != INVALID_ID {
        bd.defs_count = bd.defs_count + 1;
    }
}

// Get variable value - O(1) direct array access
fn BlockDefs_get(bd: *BlockDefs, var_idx: i64) i64 {
    if var_idx < 0 or var_idx >= bd.values_cap { return INVALID_ID; }
    return (bd.values + var_idx).*;
}

// ============================================================================
// IR to SSA Block Mapping
// ============================================================================

struct BlockMapping {
    ir_block: i64,        // IR block index
    ssa_block: i64,       // SSA block ID
}

// ============================================================================
// SSA Builder
// ============================================================================

struct SSABuilder {
    // The SSA function being built
    func: *Func,

    // Current block
    current_block: i64,

    // Variable definitions for current block
    current_defs: *BlockDefs,

    // All block definitions (for phi insertion)
    all_defs: *BlockDefs,     // Array of BlockDefs
    all_defs_count: i64,
    all_defs_cap: i64,

    // IR to SSA block mapping
    block_map: *BlockMapping,
    block_map_count: i64,
    block_map_cap: i64,
    // OPTIMIZATION: Sparse index for O(1) lookup
    block_map_index: *i64,      // index[ir_block] = position in block_map (or -1)
    block_map_index_cap: i64,

    // IR node to SSA value mapping (prevents duplicate conversion)
    // OPTIMIZED: Direct-indexed i64 array for O(1) lookup
    // Index = ir_idx - ir_nodes_start, Value = SSA value_id
    node_values: *i64,
    node_values_cap: i64,

    // Storage for per-block i64 arrays (direct-indexed by var_idx)
    var_storage: *i64,
    var_storage_used: i64,
    var_storage_cap: i64,

    // === FwdRef Pattern State ===
    // Following Zig ssa_builder.zig: fwd_vars and defvars
    fwd_vars: *VarDef,        // FwdRefs created in current block (var_idx→value_id)
    fwd_vars_count: i64,
    fwd_vars_cap: i64,
    // OPTIMIZATION: Sparse index for O(1) lookup
    fwd_vars_index: *i64,     // index[var_idx] = value_id (or INVALID_ID)
    fwd_vars_index_cap: i64,

    // defvars: per-block variable→value mapping for phi insertion
    // Uses all_defs array (BlockDefs already tracks this)

    // === IR References (set before build) ===
    ir_nodes: *IRNode,        // Global IR nodes array
    ir_nodes_start: i64,      // Start index for this function
    ir_nodes_end: i64,        // End index for this function
    ir_locals: *IRLocal,      // Global IR locals array
    ir_locals_start: i64,     // Start index for this function
    ir_locals_count: i64,     // Number of locals
    source: *u8,              // Source text for function names
    source_len: i64,

    // === Type Registry (for struct checks) ===
    // Following Zig ssa_builder.zig line 49: type_registry: *TypeRegistry
    type_registry: *TypeRegistry,

    // === Register allocation state ===
    // Simple round-robin allocation matching original main.cot behavior
    next_reg: i64,            // Next register to assign (X1-X28)
    max_reg: i64,             // Maximum register (28 = X28)
}

// ============================================================================
// Builder Initialization
// ============================================================================

// SSABuilder owns its storage (matches bootstrap Zig pattern)
// Allocates all arrays directly into struct fields via module storage

impl SSABuilder {

    fn init(self: *SSABuilder, f: *Func) {
        // Allocate module storage on first use
        SSABuilder_allocateStorage();
    
        self.func = f;
        self.current_block = INVALID_BLOCK;
        self.current_defs = null;
    
        // Use module-allocated storage
        self.all_defs = sb_all_defs;
        self.all_defs_count = 0;
        self.all_defs_cap = sb_all_defs_cap;
    
        // CRITICAL: Reset all_defs for new function (stale initialized flags cause crashes)
        var bd_idx: i64 = 0;
        while bd_idx < sb_all_defs_cap {
            let bd: *BlockDefs = sb_all_defs + bd_idx;
            BlockDefs_clear(bd);
            bd_idx = bd_idx + 1;
        }
    
        self.block_map = sb_block_map;
        self.block_map_count = 0;
        self.block_map_cap = sb_block_map_cap;
        // OPTIMIZATION: Sparse index for O(1) lookup
        self.block_map_index = sb_block_map_index;
        self.block_map_index_cap = sb_block_map_index_cap;
    
        // CRITICAL: Reset block_map_index for new function (stale entries cause crashes)
        var bmi: i64 = 0;
        while bmi < sb_block_map_index_cap {
            (sb_block_map_index + bmi).* = -1;
            bmi = bmi + 1;
        }
    
        self.node_values = sb_node_values;
        self.node_values_cap = sb_node_values_cap;
    
        // CRITICAL: Reset node_values for new function (stale entries cause crashes)
        var nvi: i64 = 0;
        while nvi < sb_node_values_cap {
            (sb_node_values + nvi).* = INVALID_ID;
            nvi = nvi + 1;
        }
    
        self.var_storage = sb_var_storage;
        self.var_storage_used = 0;
        self.var_storage_cap = sb_var_storage_cap;
    
        // CRITICAL: Reset var_storage for new function (stale entries cause crashes)
        var vsi: i64 = 0;
        while vsi < sb_var_storage_cap {
            (sb_var_storage + vsi).* = INVALID_ID;
            vsi = vsi + 1;
        }
    
        // FwdRef pattern state
        self.fwd_vars = sb_fwd_vars;
        self.fwd_vars_count = 0;
        self.fwd_vars_cap = sb_fwd_vars_cap;
        // OPTIMIZATION: Sparse index for O(1) lookup
        self.fwd_vars_index = sb_fwd_vars_index;
        self.fwd_vars_index_cap = sb_fwd_vars_index_cap;
    
        // CRITICAL: Reset fwd_vars_index for new function (stale entries cause crashes)
        var fvi: i64 = 0;
        while fvi < sb_fwd_vars_index_cap {
            (sb_fwd_vars_index + fvi).* = INVALID_ID;
            fvi = fvi + 1;
        }
    
        // IR references (set later via SSABuilder_setIRNodes/Locals/Source)
        self.ir_nodes = null;
        self.ir_nodes_start = 0;
        self.ir_nodes_end = 0;
        self.ir_locals = null;
        self.ir_locals_start = 0;
        self.ir_locals_count = 0;
        self.source = null;
        self.source_len = 0;
        self.type_registry = null;
    
        // Register allocation - simple round-robin matching original main.cot
        self.next_reg = X1;          // Start at X1 (X0 is reserved for return value)
        self.max_reg = 28;           // Max is X28 (29=FP, 30=LR, 31=SP/XZR)
    }

    fn setTypeRegistry(self: *SSABuilder, reg: *TypeRegistry) {
        self.type_registry = reg;
    }

    fn assignReg(self: *SSABuilder, v: *Value) i64 {
        // Skip X16, X17, X18 if we're in that range (reserved: linker scratch + platform)
        if self.next_reg >= 16 and self.next_reg <= 18 {
            self.next_reg = 19;  // Skip to X19
        }
    
        let reg: i64 = self.next_reg;
        v.reg = reg;
    
        // Advance to next allocatable register
        self.next_reg = self.next_reg + 1;
        // Skip X16, X17, X18 if we wrap into that range
        if self.next_reg == 16 {
            self.next_reg = 19;  // Skip to X19
        }
        if self.next_reg > self.max_reg {
            self.next_reg = X1;  // Wrap around
        }
        return reg;
    }

    fn setIRNodes(self: *SSABuilder,
                        ir_nodes: *IRNode, ir_nodes_start: i64, ir_nodes_count: i64) {
        self.ir_nodes = ir_nodes;
        self.ir_nodes_start = ir_nodes_start;
        self.ir_nodes_end = ir_nodes_start + ir_nodes_count;
    }

    fn setIRLocals(self: *SSABuilder,
                        ir_locals: *IRLocal, ir_locals_start: i64, ir_locals_count: i64) {
        self.ir_locals = ir_locals;
        self.ir_locals_start = ir_locals_start;
        self.ir_locals_count = ir_locals_count;
    }

    fn setSource(self: *SSABuilder, source: *u8, source_len: i64) {
        self.source = source;
        self.source_len = source_len;
    }

    fn allocDefs(self: *SSABuilder, count: i64) *i64 {
        // Grow var_storage if needed
        if self.var_storage_used + count > self.var_storage_cap {
            let new_cap: i64 = self.var_storage_cap * 2;
            // Update both local and global pointers
            let old_ptr: *i64 = sb_var_storage;
            sb_var_storage = realloc_i64(old_ptr, sb_var_storage_cap, new_cap);
            sb_var_storage_cap = new_cap;
            self.var_storage = sb_var_storage;
            self.var_storage_cap = new_cap;
        }
        let result: *i64 = self.var_storage + self.var_storage_used;
        self.var_storage_used = self.var_storage_used + count;
        return result;
    }

    fn newBlock(self: *SSABuilder, kind: BlockKind) i64 {
        let block_id: i64 = self.func.newBlock(kind);
        self.setBlock(block_id);
        return block_id;
    }

    fn currentBlock(self: *SSABuilder) i64 {
        return self.current_block;
    }

    fn setBlock(self: *SSABuilder, block_id: i64) {
        // Clear per-block fwd_vars (like Zig's startBlock clears fwd_vars)
        // This prevents FwdRefs from accumulating across blocks
        self.fwd_vars_count = 0;
    
        self.current_block = block_id;
        self.func.setBlock(block_id);
    
        // Grow all_defs if block_id is beyond capacity
        // This ensures we can use direct indexing: all_defs[block_id]
        while block_id >= self.all_defs_cap {
            let new_cap: i64 = self.all_defs_cap * 2;
            let blockdefs_size: i64 = @sizeOf(BlockDefs);
            let old_ptr: *u8 = @ptrCast(*u8, sb_all_defs);
            let new_ptr: *u8 = realloc_sized(old_ptr, sb_all_defs_cap, new_cap, blockdefs_size);
            sb_all_defs = @ptrCast(*BlockDefs, new_ptr);
    
            // Initialize new entries as uninitialized
            var i: i64 = sb_all_defs_cap;
            while i < new_cap {
                let bd: *BlockDefs = sb_all_defs + i;
                BlockDefs_clear(bd);
                i = i + 1;
            }
    
            sb_all_defs_cap = new_cap;
            self.all_defs = sb_all_defs;
            self.all_defs_cap = new_cap;
        }
    
        // Direct index: all_defs[block_id]
        let bd: *BlockDefs = self.all_defs + block_id;
    
        // Check if already initialized (like Zig's getOrPut)
        if bd.initialized {
            self.current_defs = bd;
            return;
        }
    
        // First visit to this block - initialize it with direct-indexed i64 array
        let storage: *i64 = self.allocDefs(MAX_VAR_DEFS);
        BlockDefs_init(bd, block_id, storage, MAX_VAR_DEFS);
        self.current_defs = bd;
        self.all_defs_count = self.all_defs_count + 1;
    }

    fn mapBlock(self: *SSABuilder, ir_block: i64, ssa_block: i64) {
        // Grow block_map if needed - use @sizeOf for correct size
        if self.block_map_count >= self.block_map_cap {
            let new_cap: i64 = self.block_map_cap * 2;
            let blockmap_size: i64 = @sizeOf(BlockMapping);
            let old_ptr: *u8 = @ptrCast(*u8, sb_block_map);
            let new_ptr: *u8 = realloc_sized(old_ptr, sb_block_map_cap, new_cap, blockmap_size);
            sb_block_map = @ptrCast(*BlockMapping, new_ptr);
            sb_block_map_cap = new_cap;
            self.block_map = sb_block_map;
            self.block_map_cap = new_cap;
        }
        let m: *BlockMapping = self.block_map + self.block_map_count;
        m.ir_block = ir_block;
        m.ssa_block = ssa_block;
    
        // OPTIMIZATION: Update sparse index for O(1) lookup
        // Grow index if needed
        if ir_block >= self.block_map_index_cap {
            let new_cap: i64 = ir_block * 2 + 16;
            let old_ptr: *i64 = sb_block_map_index;
            let new_ptr: *i64 = realloc_i64(old_ptr, sb_block_map_index_cap, new_cap);
            // Initialize new entries to -1
            var ni: i64 = sb_block_map_index_cap;
            while ni < new_cap {
                (new_ptr + ni).* = -1;
                ni = ni + 1;
            }
            sb_block_map_index = new_ptr;
            sb_block_map_index_cap = new_cap;
            self.block_map_index = new_ptr;
            self.block_map_index_cap = new_cap;
        }
        // Store position in block_map
        (self.block_map_index + ir_block).* = self.block_map_count;
    
        self.block_map_count = self.block_map_count + 1;
    }

    fn getSSABlock(self: *SSABuilder, ir_block: i64) i64 {
        // Use sparse index for O(1) lookup
        if ir_block >= 0 and ir_block < self.block_map_index_cap and self.block_map_index != null {
            let pos: i64 = (self.block_map_index + ir_block).*;
            if pos >= 0 and pos < self.block_map_count {
                let m: *BlockMapping = self.block_map + pos;
                return m.ssa_block;
            }
        }
        return INVALID_BLOCK;
    }

    fn setVar(self: *SSABuilder, var_idx: i64, value_id: i64) {
        if self.current_defs != null {
            BlockDefs_set(self.current_defs, var_idx, value_id);
        }
    }

    fn getVar(self: *SSABuilder, var_idx: i64) i64 {
        if self.current_defs != null {
            return BlockDefs_get(self.current_defs, var_idx);
        }
        return INVALID_ID;
    }

    fn initFwdVars(self: *SSABuilder, storage: *VarDef, cap: i64) {
        self.fwd_vars = storage;
        self.fwd_vars_count = 0;
        self.fwd_vars_cap = cap;
    }

    fn clearFwdVars(self: *SSABuilder) {
        // OPTIMIZATION: Clear sparse index entries for O(1) lookup reset
        if self.fwd_vars_index != null {
            var i: i64 = 0;
            while i < self.fwd_vars_count {
                let fv: *VarDef = self.fwd_vars + i;
                if fv.var_idx >= 0 and fv.var_idx < self.fwd_vars_index_cap {
                    (self.fwd_vars_index + fv.var_idx).* = INVALID_ID;
                }
                i = i + 1;
            }
        }
        self.fwd_vars_count = 0;
    }

    fn getFwdVar(self: *SSABuilder, var_idx: i64) i64 {
        // Use sparse index for O(1) lookup
        if var_idx >= 0 and var_idx < self.fwd_vars_index_cap and self.fwd_vars_index != null {
            return (self.fwd_vars_index + var_idx).*;
        }
        return INVALID_ID;
    }

    fn setFwdVar(self: *SSABuilder, var_idx: i64, value_id: i64) {
        // Grow fwd_vars if needed - use @sizeOf for correct size
        if self.fwd_vars_count >= self.fwd_vars_cap {
            let new_cap: i64 = self.fwd_vars_cap * 2;
            let vardef_size: i64 = @sizeOf(VarDef);
            let old_ptr: *u8 = @ptrCast(*u8, sb_fwd_vars);
            let new_ptr: *u8 = realloc_sized(old_ptr, sb_fwd_vars_cap, new_cap, vardef_size);
            sb_fwd_vars = @ptrCast(*VarDef, new_ptr);
            sb_fwd_vars_cap = new_cap;
            self.fwd_vars = sb_fwd_vars;
            self.fwd_vars_cap = new_cap;
        }
        let fv: *VarDef = self.fwd_vars + self.fwd_vars_count;
        fv.var_idx = var_idx;
        fv.value_id = value_id;
        self.fwd_vars_count = self.fwd_vars_count + 1;
    
        // OPTIMIZATION: Update sparse index for O(1) lookup
        // Grow index if needed
        if var_idx >= self.fwd_vars_index_cap {
            let new_cap: i64 = var_idx * 2 + 16;
            let old_ptr: *i64 = sb_fwd_vars_index;
            let new_ptr: *i64 = realloc_i64(old_ptr, sb_fwd_vars_index_cap, new_cap);
            // Initialize new entries to INVALID_ID
            var ni: i64 = sb_fwd_vars_index_cap;
            while ni < new_cap {
                (new_ptr + ni).* = INVALID_ID;
                ni = ni + 1;
            }
            sb_fwd_vars_index = new_ptr;
            sb_fwd_vars_index_cap = new_cap;
            self.fwd_vars_index = new_ptr;
            self.fwd_vars_index_cap = new_cap;
        }
        // Store value directly in index (not position)
        (self.fwd_vars_index + var_idx).* = value_id;
    }

    fn variable(self: *SSABuilder, local_idx: i64, type_idx: i64) *Value {
        // 1. Check current block's definitions
        let val_id: i64 = self.getVar(local_idx);
        if val_id != INVALID_ID {
            return self.func.getValue(val_id);
        }
    
        // 2. Check if we already created a FwdRef for this
        let fwd_id: i64 = self.getFwdVar(local_idx);
        if fwd_id != INVALID_ID {
            return self.func.getValue(fwd_id);
        }
    
        // 3. Create forward reference - will be resolved later by insertPhis
        let fwd_ref: *Value = self.func.newValue(Op.FwdRef, type_idx);
        fwd_ref.aux_int = local_idx;  // Remember which variable
    
        // 4. Cache to coalesce multiple uses in same block
        self.setFwdVar(local_idx, fwd_ref.id);
    
        return fwd_ref;
    }

    fn ensureDefvar(self: *SSABuilder, block_id: i64, local_idx: i64, value_id: i64) {
        let bd: *BlockDefs = self.getBlockDefs(block_id);
        if bd == null { return; }
    
        // Only set if not already defined (don't override real definitions with FwdRefs)
        let existing: i64 = BlockDefs_get(bd, local_idx);
        if existing == INVALID_ID {
            BlockDefs_set(bd, local_idx, value_id);
        }
    }

    fn lookupVarOutgoing(self: *SSABuilder, block_id: i64, local_idx: i64, type_idx: i64,
                                    fwd_refs: *I64List) i64 {
        var cur_id: i64 = block_id;
    
        // Walk backwards through single-predecessor chains
        while true {
            // Check if block defines this variable
            let bd: *BlockDefs = self.getBlockDefs(cur_id);
            if bd != null {
                let val_id: i64 = BlockDefs_get(bd, local_idx);
                if val_id != INVALID_ID {
                    return val_id;
                }
            }
    
            // Get block to check predecessors
            let blk: *Block = self.func.getBlock(cur_id);
            if blk == null { break; }
    
            // Single predecessor? Keep walking back
            if blk.preds.count == 1 {
                cur_id = blk.getPred(0);
                continue;
            }
    
            // Multiple predecessors or no predecessors (entry) - stop walking
            break;
        }
    
        // Create a new FwdRef in the block we walked back to (cur_id), NOT current block
        // This matches Zig: self.func.newValue(.fwd_ref, type_idx, cur, self.cur_pos)
        // Temporarily switch to cur_id block to create value there
        let saved_block: i64 = self.func.current_block;
        self.func.setBlock(cur_id);
        let new_fwd: *Value = self.func.newValue(Op.FwdRef, type_idx);
        new_fwd.aux_int = local_idx;
        self.func.setBlock(saved_block);  // Restore
    
        // Store in defvars so we don't create duplicate
        self.ensureDefvar(cur_id, local_idx, new_fwd.id);
    
        // CRITICAL: Add to work list so it gets processed
        i64list_append(fwd_refs, new_fwd.id);
    
        return new_fwd.id;
    }

    fn reorderPhis(self: *SSABuilder) {
        var block_idx: i64 = 0;
        while block_idx < self.func.blocks_count {
            let blk: *Block = self.func.getBlock(block_idx);
    
            // Count phis and non-phis
            var phi_count: i64 = 0;
            var i: i64 = 0;
            while i < blk.values_count {
                let v: *Value = self.func.getValue(blk.values_start + i);
                if v.op == Op.Phi {
                    phi_count = phi_count + 1;
                }
                i = i + 1;
            }
    
            // If all values are phis or no phis, nothing to do
            if phi_count == 0 or phi_count == blk.values_count {
                block_idx = block_idx + 1;
                continue;
            }
    
            // Need to reorder: use dynamic list
            var temp_ids: I64List = undefined;
            i64list_init(&temp_ids);
    
            // First pass: collect phis
            i = 0;
            while i < blk.values_count {
                let v: *Value = self.func.getValue(blk.values_start + i);
                if v.op == Op.Phi {
                    i64list_append(&temp_ids, blk.values_start + i);
                }
                i = i + 1;
            }
    
            // Second pass: collect non-phis
            i = 0;
            while i < blk.values_count {
                let v: *Value = self.func.getValue(blk.values_start + i);
                if v.op != Op.Phi {
                    i64list_append(&temp_ids, blk.values_start + i);
                }
                i = i + 1;
            }
    
            // Note: The values themselves don't move, just their IDs
            // This is a simplification - real implementation would update the block's value list
            // For now, we rely on codegen handling phis first regardless of order
    
            i64list_deinit(&temp_ids);
            block_idx = block_idx + 1;
        }
    }

    fn insertPhis(self: *SSABuilder) {
        // Work list of FwdRef value IDs to process (dynamic list)
        var fwd_refs: I64List = undefined;
        i64list_init(&fwd_refs);
    
        // Collect initial FwdRef values and treat them as definitions
        var block_idx: i64 = 0;
        while block_idx < self.func.blocks_count {
            let blk: *Block = self.func.getBlock(block_idx);
            var i: i64 = 0;
            while i < blk.values_count {
                let val_id: i64 = blk.values_start + i;
                let v: *Value = self.func.getValue(val_id);
                if v.op == Op.FwdRef {
                    // Add to work list
                    i64list_append(&fwd_refs, val_id);
                    // Treat FwdRefs as definitions in their block
                    let local_idx: i64 = v.aux_int;
                    self.ensureDefvar(block_idx, local_idx, val_id);
                }
                i = i + 1;
            }
            block_idx = block_idx + 1;
        }
    
        // Temporary storage for incoming values (dynamic list)
        var args: I64List = undefined;
        i64list_init(&args);
    
        // Safety: limit iterations to prevent infinite loops
        // Max iterations = initial FwdRefs * max_block_count (generous upper bound)
        let initial_fwd_refs: i64 = fwd_refs.count;
        let max_iters: i64 = (fwd_refs.count + 1) * (self.func.blocks_count + 1) * 2;
        var iter_count: i64 = 0;
    
        // (Debug output removed for performance)
    
        // Process FwdRefs iteratively until the work list is empty
        while fwd_refs.count > 0 {
            iter_count = iter_count + 1;
            if iter_count > max_iters {
                // Bail out to prevent infinite loop - print warning
                // print("  WARNING: phi insertion bailed out at iter="); print(iter_count); print(" remaining="); print(fwd_refs.count); print("\n");
                break;
            }
    
            // Pop from work list
            let fwd_id: i64 = i64list_pop(&fwd_refs);
            let fwd: *Value = self.func.getValue(fwd_id);
    
            // Get block directly from Value's block_id field (O(1) instead of O(n*m) search)
            // This matches Zig: const block = fwd.block orelse continue;
            let fwd_block_id: i64 = fwd.block_id;
            if fwd_block_id < 0 { continue; }
    
            let blk: *Block = self.func.getBlock(fwd_block_id);
    
            // Entry block should never have FwdRef (variable used before defined)
            if fwd_block_id == 0 {
                continue;
            }
    
            // No predecessors? Skip (unreachable block)
            if blk.preds.count == 0 {
                continue;
            }
    
            let local_idx: i64 = fwd.aux_int;
    
            // Find variable value on each predecessor
            i64list_clear(&args);
            var pred_idx: i64 = 0;
            while pred_idx < blk.preds.count {
                let pred_block_id: i64 = blk.getPred(pred_idx);
                let val_id: i64 = self.lookupVarOutgoing(pred_block_id, local_idx, fwd.type_idx,
                                                               &fwd_refs);
                i64list_append(&args, val_id);
                pred_idx = pred_idx + 1;
            }
    
            // Decide if we need a phi or not
            // We need a phi if there are two different args (excluding self-references)
            var witness: i64 = INVALID_ID;
            var need_phi: bool = false;
    
            var ai: i64 = 0;
            while ai < args.count {
                let a: i64 = i64list_get(&args, ai);
                if a == fwd_id {
                    ai = ai + 1;
                    continue;  // Self-reference, skip
                }
                if witness == INVALID_ID {
                    witness = a;  // First witness
                } else if a != witness {
                    need_phi = true;  // Two different values, need phi
                    break;
                }
                ai = ai + 1;
            }
    
            if need_phi {
                // Convert to Phi with all incoming values
                fwd.op = Op.Phi;
                ai = 0;
                while ai < args.count {
                    let arg_val: *Value = self.func.getValue(i64list_get(&args, ai));
                    fwd.addArg(arg_val);
                    ai = ai + 1;
                }
            } else if witness != INVALID_ID {
                // One witness (excluding self). Make it a copy.
                fwd.op = Op.Copy;
                let witness_val: *Value = self.func.getValue(witness);
                fwd.addArg(witness_val);
            }
            // If no witness at all (all self-references), leave as FwdRef
            // This is an error but will be caught later
        }
    
        // Clean up dynamic lists
        i64list_deinit(&args);
        i64list_deinit(&fwd_refs);
    
        // Reorder all blocks to ensure phis are at the start
        self.reorderPhis();
    }

    fn cacheNode(self: *SSABuilder, node_idx: i64, value_id: i64) {
        let rel_idx: i64 = node_idx - self.ir_nodes_start;
        if rel_idx < 0 or rel_idx >= self.node_values_cap {
            return;  // Out of bounds
        }
        (self.node_values + rel_idx).* = value_id;
    }

    fn getCached(self: *SSABuilder, node_idx: i64) i64 {
        let rel_idx: i64 = node_idx - self.ir_nodes_start;
        if rel_idx < 0 or rel_idx >= self.node_values_cap {
            return INVALID_ID;
        }
        return (self.node_values + rel_idx).*;
    }

    fn emitConstInt(self: *SSABuilder, value: i64, type_idx: i64) *Value {
        return self.func.emitConstInt(value, type_idx);
    }

    fn emitConstBool(self: *SSABuilder, value: bool) *Value {
        return self.func.emitConstBool(value);
    }

    fn emitBinary(self: *SSABuilder, op: Op, left: *Value, right: *Value, type_idx: i64) *Value {
        return self.func.emitBinary(op, left, right, type_idx);
    }

    fn emitUnary(self: *SSABuilder, op: Op, operand: *Value, type_idx: i64) *Value {
        return self.func.emitUnary(op, operand, type_idx);
    }

    fn emitLoadLocal(self: *SSABuilder, local_idx: i64) *Value {
        return self.func.emitLoadLocal(local_idx);
    }

    fn emitStoreLocal(self: *SSABuilder, local_idx: i64, val: *Value) *Value {
        return self.func.emitStoreLocal(local_idx, val);
    }

    fn emitReturn(self: *SSABuilder, val: *Value) *Value {
        return self.func.emitReturn(val);
    }

    fn emitAlloca(self: *SSABuilder, size: i64, align: i64, type_idx: i64) *Value {
        // In cot0, we use LocalAddr to get address of a local variable
        // Add a new local to the function and return its address
        let local_idx: i64 = self.func.addLocal(0, 0, type_idx, true);
        let local: *Local = self.func.getLocal(local_idx);
        local.size = size;
        local.align = align;
    
        let addr: *Value = self.func.newValue(Op.LocalAddr, type_idx);
        addr.aux_int = local_idx;
        return addr;
    }

    fn emitCast(self: *SSABuilder, val: *Value, src_size: i64, dst_size: i64, is_signed: bool, dst_type: i64) *Value {
        // Same size - no conversion needed
        if src_size == dst_size {
            return val;
        }
    
        // Extension (smaller to larger)
        if src_size < dst_size {
            var op: Op = Op.ZeroExt8to64;  // Default
            if is_signed {
                if src_size == 1 {
                    op = Op.SignExt8to64;
                } else if src_size == 2 {
                    op = Op.SignExt16to64;
                } else if src_size == 4 {
                    op = Op.SignExt32to64;
                }
            } else {
                if src_size == 1 {
                    op = Op.ZeroExt8to64;
                } else if src_size == 2 {
                    op = Op.ZeroExt16to64;
                } else if src_size == 4 {
                    op = Op.ZeroExt32to64;
                }
            }
            return self.func.emitUnary(op, val, dst_type);
        }
    
        // Truncation (larger to smaller)
        var op: Op = Op.Trunc64to8;  // Default
        if dst_size == 1 {
            op = Op.Trunc64to8;
        } else if dst_size == 2 {
            op = Op.Trunc64to16;
        } else if dst_size == 4 {
            op = Op.Trunc64to32;
        }
        return self.func.emitUnary(op, val, dst_type);
    }

    fn emitIf(self: *SSABuilder, cond: *Value, then_block: i64, else_block: i64) {
        self.func.emitIf(cond, then_block, else_block);
    }

    fn emitJump(self: *SSABuilder, target: i64) {
        self.func.emitJump(target);
    }

    fn emitReturn_block(self: *SSABuilder) {
        self.func.emitReturnBlock();
    }

    fn emitPhi(self: *SSABuilder, type_idx: i64) *Value {
        let v: *Value = self.func.newValue(Op.Phi, type_idx);
        return v;
    }

    fn phiAddArg(self: *SSABuilder, phi: *Value, val: *Value, pred_block: i64) {
        // For simplicity, store args directly
        // In full implementation, args are ordered by predecessor
        phi.addArg(val);
    }

    fn build(self: *SSABuilder) i64 {
        // Step 1: Find max block_id
    
        // Step 1: Find max block_id in IR nodes to know how many blocks we need
        var max_block_id: i64 = 0;
        var ir_idx: i64 = self.ir_nodes_start;
        while ir_idx < self.ir_nodes_end {
            let ir_node: *IRNode = self.ir_nodes + ir_idx;
            if ir_node.block_id > max_block_id {
                max_block_id = ir_node.block_id;
            }
            ir_idx = ir_idx + 1;
        }
    
        // Step 2: Create SSA blocks
    
        // Step 2: Create SSA blocks
        // Entry block (block 0)
        let entry: i64 = self.func.newEntryBlock();
        self.setBlock(entry);
        self.mapBlock(0, entry);
    
        // Create additional blocks (1 through max_block_id)
        var block_idx: i64 = 1;
        while block_idx <= max_block_id {
            let new_block: i64 = self.func.newBlock(BlockKind.Plain);
            self.mapBlock(block_idx, new_block);
            block_idx = block_idx + 1;
        }
    
        // Step 3: Register locals
    
        // Step 3: Register locals (params first, then regular locals)
        // Copy from IR locals to SSA locals
        var local_idx: i64 = self.ir_locals_start;
        let locals_end: i64 = self.ir_locals_start + self.ir_locals_count;
        var ssa_local_idx: i64 = 0;
        while local_idx < locals_end {
            let ir_local: *IRLocal = self.ir_locals + local_idx;
            if ir_local.is_param {
                self.func.addParam(ir_local.name_start, ir_local.name_len,
                              ir_local.type_idx);
            } else {
                self.func.addLocal(ir_local.name_start, ir_local.name_len,
                              ir_local.type_idx, ir_local.is_mutable);
            }
            // Copy size from IR local to SSA local
            let ssa_local: *Local = self.func.getLocal(ssa_local_idx);
            ssa_local.size = ir_local.size;
            local_idx = local_idx + 1;
            ssa_local_idx = ssa_local_idx + 1;
        }
        // Step 4: Handle parameters
        // Step 4: Handle parameters with 3-phase approach (BUG-056 fix)
        // Following Zig's ssa_builder.zig lines 111-242
        //
        // CRITICAL: Emit ALL Arg ops FIRST, then stores.
        // This ensures ABI registers (x0-x7) aren't overwritten before being captured.
        //
        // Phase 1: Create ALL Arg ops (captures all ABI register values)
        // Phase 2: Create slice_make ops for string params (not yet implemented)
        // Phase 3: Store all params to stack slots
        //
        // String/slice params take TWO registers (ptr + len), so we track the
        // physical register index separately from the logical param index.
    
        // Dynamic lists to track params across phases (store value IDs, not pointers)
        var arg_value_ids: I64List = undefined;
        var arg_local_indices: I64List = undefined;
        i64list_init(&arg_value_ids);
        i64list_init(&arg_local_indices);
    
        // Phase 1: Create ALL Arg ops first
        // phys_reg_idx tracks the physical ABI register (x0, x1, ..., or stack slot)
        var phys_reg_idx: i64 = 0;
        var param_local_idx: i64 = 0;
        while param_local_idx < self.func.locals_count {
            let param_local: *Local = self.func.getLocal(param_local_idx);
            if param_local.is_param {
                // Check if this is a string parameter (needs 2 registers: ptr, len)
                let is_string_param: bool = param_local.type_idx == TYPE_STRING;
    
                if is_string_param {
                    // String param: create TWO Arg ops (ptr in reg N, len in reg N+1)
                    // Then combine them with StringMake
                    //
                    // CRITICAL: Use IDs, not pointers - Func_newValue can realloc!
    
                    // Arg for ptr
                    let ptr_arg_val: *Value = self.func.newValue(Op.Arg, TYPE_I64);
                    let ptr_arg_id: i64 = ptr_arg_val.id;  // Save ID before potential realloc
                    ptr_arg_val.aux_int = phys_reg_idx;
                    if phys_reg_idx < 8 {
                        ptr_arg_val.reg = X0 + phys_reg_idx;
                    }
                    phys_reg_idx = phys_reg_idx + 1;
    
                    // Arg for len
                    let len_arg_val: *Value = self.func.newValue(Op.Arg, TYPE_I64);
                    let len_arg_id: i64 = len_arg_val.id;  // Save ID before potential realloc
                    len_arg_val.aux_int = phys_reg_idx;
                    if phys_reg_idx < 8 {
                        len_arg_val.reg = X0 + phys_reg_idx;
                    }
                    phys_reg_idx = phys_reg_idx + 1;
    
                    // Create StringMake to combine ptr and len
                    // Re-fetch pointers since Func_newValue may have reallocated
                    let str_make_val: *Value = self.func.newValue(Op.StringMake, TYPE_STRING);
                    let fresh_ptr_val: *Value = self.func.getValue(ptr_arg_id);
                    let fresh_len_val: *Value = self.func.getValue(len_arg_id);
                    str_make_val.addArg(fresh_ptr_val);
                    str_make_val.addArg(fresh_len_val);
    
                    // Save StringMake result for Phase 3 (not the raw Arg)
                    i64list_append(&arg_value_ids, str_make_val.id);
                    i64list_append(&arg_local_indices, param_local_idx);
                } else {
                    // Non-string param: create single Arg op
                    // ARM64 ABI: x0-x7 for first 8 args, stack for rest
                    let arg_val: *Value = self.func.newValue(Op.Arg, TYPE_I64);
                    arg_val.aux_int = phys_reg_idx;  // Physical register index, not logical param index
                    if phys_reg_idx < 8 {
                        // Register argument - pre-assign to X0-X7
                        arg_val.reg = X0 + phys_reg_idx;
                    }
                    // Stack arguments (phys_reg_idx >= 8) have reg=-1, will be assigned by regalloc
    
                    // Save for Phase 3
                    i64list_append(&arg_value_ids, arg_val.id);
                    i64list_append(&arg_local_indices, param_local_idx);
    
                    phys_reg_idx = phys_reg_idx + 1;
                }
            }
            param_local_idx = param_local_idx + 1;
        }
    
        // Phase 2: String params handled above with StringMake
    
        // Phase 3: Store all params to stack slots
        // This happens AFTER all Arg ops are created
        var i: i64 = 0;
        while i < arg_value_ids.count {
            // CRITICAL: Use IDs throughout to avoid stale pointers after realloc
            let arg_val_id: i64 = i64list_get(&arg_value_ids, i);
            let local_idx: i64 = i64list_get(&arg_local_indices, i);
            let param_local: *Local = self.func.getLocal(local_idx);
    
            // Create LocalAddr value for the stack slot
            let addr_val: *Value = self.func.newValue(Op.LocalAddr, TYPE_I64);
            let addr_val_id: i64 = addr_val.id;  // Save ID
            addr_val.aux_int = local_idx;
    
            // Re-fetch arg_val after potential realloc
            let arg_val: *Value = self.func.getValue(arg_val_id);
    
            // Handle string params specially: store both ptr and len components
            if arg_val.op == Op.StringMake and arg_val.args.count >= 2 {
                // String param: store ptr at offset 0, len at offset 8
                // Get the arg IDs from the StringMake value
                let ptr_val_id: i64 = arg_val.getArg(0);
                let len_val_id: i64 = arg_val.getArg(1);
    
                // Store ptr to base address
                let store_ptr: *Value = self.func.newValue(Op.Store, TYPE_VOID);
                // Re-fetch all needed values after Func_newValue
                let fresh_addr: *Value = self.func.getValue(addr_val_id);
                let fresh_ptr: *Value = self.func.getValue(ptr_val_id);
                store_ptr.addArg2(fresh_addr, fresh_ptr);
    
                // Create addr + 8 for len using OffPtr (offset pointer)
                let addr_plus_8: *Value = self.func.newValue(Op.OffPtr, TYPE_I64);
                let addr_plus_8_id: i64 = addr_plus_8.id;
                addr_plus_8.aux_int = 8;  // offset
                // Re-fetch addr after potential realloc
                let fresh_addr2: *Value = self.func.getValue(addr_val_id);
                addr_plus_8.addArg(fresh_addr2);  // base pointer
    
                // Store len at offset 8
                let store_len: *Value = self.func.newValue(Op.Store, TYPE_VOID);
                // Re-fetch values after Func_newValue
                let fresh_addr_plus_8: *Value = self.func.getValue(addr_plus_8_id);
                let fresh_len: *Value = self.func.getValue(len_val_id);
                store_len.addArg2(fresh_addr_plus_8, fresh_len);
            } else {
                // BUG-019 FIX: >16B struct types are passed by reference
                // Following Zig ssa_builder.zig lines 222-238
                // param_val is a POINTER to source, use OpMove to copy
                var is_large_struct: bool = false;
                var type_size: i64 = 8;
                if self.type_registry != null {
                    type_size = self.type_registry.sizeof(param_local.type_idx);
                    let is_struct: bool = TypeInfo_isStruct(self.type_registry, param_local.type_idx);
                    if is_struct and type_size > 16 {
                        is_large_struct = true;
                    }
                }
    
                if is_large_struct {
                    // OpMove: copy from arg_val (source ptr) to addr_val (dest)
                    let move_val: *Value = self.func.newValue(Op.Move, TYPE_VOID);
                    move_val.aux_int = type_size;
                    // Re-fetch values after Func_newValue
                    let fresh_addr: *Value = self.func.getValue(addr_val_id);
                    let fresh_arg: *Value = self.func.getValue(arg_val_id);
                    move_val.addArg2(fresh_addr, fresh_arg);
                } else {
                    // Store Arg to stack slot
                    let store_val: *Value = self.func.newValue(Op.Store, TYPE_VOID);
                    // Re-fetch values after Func_newValue
                    let fresh_addr: *Value = self.func.getValue(addr_val_id);
                    let fresh_arg: *Value = self.func.getValue(arg_val_id);
                    store_val.addArg2(fresh_addr, fresh_arg);
                }
            }
    
            i = i + 1;
        }
        i64list_deinit(&arg_value_ids);
        i64list_deinit(&arg_local_indices);
    
        // Step 4.5: Initialize node_values cache for O(1) lookup
        // Size needed = number of IR nodes for this function
        let nodes_needed: i64 = self.ir_nodes_end - self.ir_nodes_start;
        // Grow if needed
        while nodes_needed > self.node_values_cap {
            let new_cap: i64 = self.node_values_cap * 2;
            let old_ptr: *i64 = sb_node_values;
            sb_node_values = realloc_i64(old_ptr, sb_node_values_cap, new_cap);
            sb_node_values_cap = new_cap;
            self.node_values = sb_node_values;
            self.node_values_cap = new_cap;
        }
        // Initialize all entries to INVALID_ID
        var nv_idx: i64 = 0;
        while nv_idx < nodes_needed {
            (self.node_values + nv_idx).* = INVALID_ID;
            nv_idx = nv_idx + 1;
        }
    
        // Step 5: Convert IR nodes to SSA
        // Track current IR block for block transitions
        var current_ir_block: i64 = 0;
        ir_idx = self.ir_nodes_start;
        while ir_idx < self.ir_nodes_end {
            let ir_node: *IRNode = self.ir_nodes + ir_idx;
    
            // Handle block transitions
            if ir_node.block_id != current_ir_block {
                current_ir_block = ir_node.block_id;
                let ssa_block: i64 = self.getSSABlock(current_ir_block);
                if ssa_block >= 0 {
                    self.setBlock(ssa_block);
                }
            }
    
            // Convert this node
            self.convertNode(ir_node, ir_idx);
    
            ir_idx = ir_idx + 1;
        }
    
        // Step 6: Insert phi nodes for forward references
        // Reference: Zig ssa_builder.zig:455
        self.insertPhis();
    
        // Step 6.5: Verify SSA form
        // Reference: Zig ssa_builder.zig:459
        self.verify();
    
        // Step 7: Emit return block
        self.func.emitReturnBlock();
    
        return 0;
    }

    fn convertNode(self: *SSABuilder, ir_node: *IRNode, ir_idx: i64) i64 {
        // Check if already converted (using node_values cache)
        let cached: i64 = self.getCached(ir_idx);
        if cached != INVALID_ID {
            return cached;
        }
    
        // Dispatch based on node kind
        var result_id: i64 = INVALID_ID;
    
        if ir_node.kind == IRNodeKind.ConstInt {
            result_id = self.convertConstInt(ir_node);
        } else if ir_node.kind == IRNodeKind.ConstString {
            result_id = self.convertConstString(ir_node);
        } else if ir_node.kind == IRNodeKind.ConstBool {
            result_id = self.convertConstBool(ir_node);
        } else if ir_node.kind == IRNodeKind.Binary {
            result_id = self.convertBinary(ir_node);
        } else if ir_node.kind == IRNodeKind.Unary {
            result_id = self.convertUnary(ir_node);
        } else if ir_node.kind == IRNodeKind.LoadLocal {
            result_id = self.convertLoadLocal(ir_node);
        } else if ir_node.kind == IRNodeKind.StoreLocal {
            result_id = self.convertStoreLocal(ir_node);
        } else if ir_node.kind == IRNodeKind.AddrLocal {
            result_id = self.convertAddrLocal(ir_node);
        } else if ir_node.kind == IRNodeKind.LoadGlobal {
            result_id = self.convertLoadGlobal(ir_node);
        } else if ir_node.kind == IRNodeKind.StoreGlobal {
            result_id = self.convertStoreGlobal(ir_node);
        } else if ir_node.kind == IRNodeKind.AddrGlobal {
            result_id = self.convertAddrGlobal(ir_node);
        } else if ir_node.kind == IRNodeKind.FuncAddr {
            result_id = self.convertFuncAddr(ir_node);
        } else if ir_node.kind == IRNodeKind.Load {
            result_id = self.convertLoad(ir_node);
        } else if ir_node.kind == IRNodeKind.Store {
            result_id = self.convertStore(ir_node);
        } else if ir_node.kind == IRNodeKind.FieldLocal {
            result_id = self.convertFieldLocal(ir_node);
        } else if ir_node.kind == IRNodeKind.FieldValue {
            result_id = self.convertFieldValue(ir_node);
        } else if ir_node.kind == IRNodeKind.StoreFieldLocal {
            result_id = self.convertStoreFieldLocal(ir_node);
        } else if ir_node.kind == IRNodeKind.StoreField {
            result_id = self.convertStoreField(ir_node);
        } else if ir_node.kind == IRNodeKind.IndexLocal {
            result_id = self.convertIndexLocal(ir_node);
        } else if ir_node.kind == IRNodeKind.IndexValue {
            result_id = self.convertIndexValue(ir_node);
        } else if ir_node.kind == IRNodeKind.StoreIndexLocal {
            result_id = self.convertStoreIndexLocal(ir_node);
        } else if ir_node.kind == IRNodeKind.StoreIndexValue {
            result_id = self.convertStoreIndexValue(ir_node);
        } else if ir_node.kind == IRNodeKind.MakeString {
            result_id = self.convertMakeString(ir_node);
        } else if ir_node.kind == IRNodeKind.MakeSlice {
            result_id = self.convertMakeSlice(ir_node);
        } else if ir_node.kind == IRNodeKind.SlicePtr {
            result_id = self.convertSlicePtr(ir_node);
        } else if ir_node.kind == IRNodeKind.SliceLen {
            result_id = self.convertSliceLen(ir_node);
        } else if ir_node.kind == IRNodeKind.Call {
            result_id = self.convertCall(ir_node);
        } else if ir_node.kind == IRNodeKind.CallIndirect {
            result_id = self.convertCallIndirect(ir_node);
        } else if ir_node.kind == IRNodeKind.StrConcat {
            result_id = self.convertStrConcat(ir_node);
        } else if ir_node.kind == IRNodeKind.Select {
            result_id = self.convertSelect(ir_node);
        } else if ir_node.kind == IRNodeKind.Branch {
            self.convertBranch(ir_node);
            result_id = INVALID_ID;
        } else if ir_node.kind == IRNodeKind.Jump {
            self.convertJump(ir_node);
            result_id = INVALID_ID;
        } else if ir_node.kind == IRNodeKind.Return {
            self.convertReturn(ir_node);
            result_id = INVALID_ID;
        }
    
        // Cache the result and set source position for debug info
        if result_id != INVALID_ID {
            self.cacheNode(ir_idx, result_id);
            // Copy source position from IR node to SSA value for DWARF line tables
            let result_val: *Value = self.func.getValue(result_id);
            result_val.pos = ir_node.pos;
        }
    
        return result_id;
    }

    fn getValue(self: *SSABuilder, value_id: i64) *Value {
        return self.func.getValue(value_id);
    }

    fn getOperandValue(self: *SSABuilder, relative_ir_idx: i64) *Value {
        let abs_idx: i64 = self.ir_nodes_start + relative_ir_idx;
        let value_id: i64 = self.getCached(abs_idx);
        return self.func.getValue(value_id);
    }

    fn convertConstInt(self: *SSABuilder, ir_node: *IRNode) i64 {
        let val: *Value = self.func.emitConstInt(ir_node.value, TYPE_I64);
        self.assignReg(val);
        return val.id;
    }

    fn convertConstString(self: *SSABuilder, ir_node: *IRNode) i64 {
        // String literal: left=str_start, right=str_len in source
        let val: *Value = self.func.emitConstString(ir_node.left, ir_node.right);
        self.assignReg(val);
        return val.id;
    }

    fn convertConstBool(self: *SSABuilder, ir_node: *IRNode) i64 {
        let val: *Value = self.func.emitConstBool(ir_node.value != 0);
        self.assignReg(val);
        return val.id;
    }

    fn convertBinary(self: *SSABuilder, ir_node: *IRNode) i64 {
        // left and right are relative IR indices
        let left_val: *Value = self.getOperandValue(ir_node.left);
        let right_val: *Value = self.getOperandValue(ir_node.right);
        let ssa_op: Op = Op_fromIRBinaryOp(ir_node.op);
        let val: *Value = self.func.emitBinary(ssa_op, left_val, right_val, TYPE_I64);
        self.assignReg(val);
        return val.id;
    }

    fn convertUnary(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Unary op: left is operand (relative IR index), op is the operator
        let operand_val: *Value = self.getOperandValue(ir_node.left);
        let ssa_op: Op = Op_fromIRUnaryOp(ir_node.op);
        let val: *Value = self.func.newValue(ssa_op, TYPE_I64);
        val.addArg(operand_val);
        self.assignReg(val);
        return val.id;
    }

    fn convertLoadLocal(self: *SSABuilder, ir_node: *IRNode) i64 {
        let loc_idx: i64 = ir_node.left;
        let load_val: *Value = self.func.emitLoadLocal(loc_idx);
        self.assignReg(load_val);
        return load_val.id;
    }

    fn convertStoreLocal(self: *SSABuilder, ir_node: *IRNode) i64 {
        let loc_idx: i64 = ir_node.left;
        // right is relative IR index of value to store
        let value_val: *Value = self.getOperandValue(ir_node.right);
    
        // BUG-019 FIX: For struct types, use Op.Move to copy the data
        // Following Zig ssa_builder.zig:228-233:
        //   const is_large_struct = (local_type_info == .struct_type) and (type_size > 16);
        //   if (is_large_struct) { const move_val = try func.newValue(.move, ...); }
        //
        // When the value is an address of a struct (from index_value, field_value, etc.),
        // we need to copy the struct data to the local, not just store the address.
        let local: *Local = self.func.getLocal(loc_idx);
        var is_struct: bool = false;
        var struct_size: i64 = 0;
        if self.type_registry != null and local.type_idx > 0 {
            let local_type: *Type = self.type_registry.get(local.type_idx);
            if local_type.kind == TypeKind.Struct {
                is_struct = true;
                struct_size = local.size;
            }
        }
    
        if is_struct and struct_size > 8 {
            // Struct type: emit Op.Move to copy data from source to dest
            // dest = address of local
            let dest_addr: *Value = self.func.newValue(Op.LocalAddr, TYPE_I64);
            dest_addr.aux_int = loc_idx;
            self.assignReg(dest_addr);
    
            // source = value_val (which should be an address for struct types)
            // Move: (dest, source) with aux_int = size
            let move_val: *Value = self.func.newValue(Op.Move, TYPE_VOID);
            move_val.aux_int = struct_size;
            move_val.addArg2(dest_addr, value_val);
            return move_val.id;
        }
    
        // Non-struct: use normal store
        let store_val: *Value = self.func.emitStoreLocal(loc_idx, value_val);
        // Stores don't need registers (they write to memory)
        return store_val.id;
    }

    fn convertAddrLocal(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Address of local variable: left = local index
        let loc_idx: i64 = ir_node.left;
        let addr_val: *Value = self.func.newValue(Op.LocalAddr, TYPE_I64);
        addr_val.aux_int = loc_idx;
        self.assignReg(addr_val);
        return addr_val.id;
    }

    fn convertLoadGlobal(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Load global variable: left = global index
        let global_idx: i64 = ir_node.left;
        let load_val: *Value = self.func.newValue(Op.GlobalLoad, ir_node.type_idx);
        load_val.aux_int = global_idx;
        self.assignReg(load_val);
        return load_val.id;
    }

    fn convertStoreGlobal(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Store to global variable: left = global index, right = value expr
        let global_idx: i64 = ir_node.left;
        let val: *Value = self.getOperandValue(ir_node.right);
        let store_val: *Value = self.func.newValue(Op.GlobalStore, TYPE_I64);
        store_val.aux_int = global_idx;
        store_val.addArg(val);
        self.assignReg(store_val);
        return store_val.id;
    }

    fn convertAddrGlobal(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Address of global variable: left = global index
        let global_idx: i64 = ir_node.left;
        let addr_val: *Value = self.func.newValue(Op.GlobalAddr, TYPE_I64);
        addr_val.aux_int = global_idx;
        self.assignReg(addr_val);
        return addr_val.id;
    }

    fn convertFuncAddr(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Address of function: func_name_start, func_name_len
        let addr_val: *Value = self.func.newValue(Op.Addr, TYPE_I64);
        // Store function name position in aux fields (same as Call)
        addr_val.aux_int = ir_node.func_name_start;
        addr_val.aux_ptr = ir_node.func_name_len;
        self.assignReg(addr_val);
        return addr_val.id;
    }

    fn convertLoad(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Load from pointer: left = ptr expr (relative IR index)
        // Use the IR node's type_idx to preserve byte types (u8, i8)
        let ptr_val: *Value = self.getOperandValue(ir_node.left);
        let load_val: *Value = self.func.newValue(Op.Load, ir_node.type_idx);
        load_val.addArg(ptr_val);
        self.assignReg(load_val);
        return load_val.id;
    }

    fn convertStore(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Store to pointer: left = ptr expr, right = value expr (relative)
        let ptr_val: *Value = self.getOperandValue(ir_node.left);
        let val_val: *Value = self.getOperandValue(ir_node.right);
        let store_val: *Value = self.func.newValue(Op.Store, TYPE_VOID);
        store_val.addArg2(ptr_val, val_val);
        // Stores don't need registers
        return store_val.id;
    }

    fn convertFieldLocal(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Access field of local struct: left = local_idx, right = offset
        // Following Zig ssa_builder.zig:1027-1061
        let loc_idx: i64 = ir_node.left;
        let offset: i64 = ir_node.right;
    
        // 1. Get address of local
        let addr_val: *Value = self.func.newValue(Op.LocalAddr, TYPE_I64);
        addr_val.aux_int = loc_idx;
    
        // 2. Add offset to get field address
        let off_val: *Value = self.func.newValue(Op.OffPtr, TYPE_I64);
        off_val.addArg(addr_val);
        off_val.aux_int = offset;
    
        // Check if result is a struct or array - if so, return address (no load)
        // Both structs and arrays are inline data, not pointers, so we return
        // the address for further field access or indexing.
        // Following Zig ssa_builder.zig:1042-1051
        if self.type_registry != null and ir_node.type_idx > 0 {
            let field_type: *Type = self.type_registry.get(ir_node.type_idx);
            if field_type != null {
                if field_type.kind == TypeKind.Struct or field_type.kind == TypeKind.Array {
                    // Nested struct or array - return address for further access
                    self.assignReg(off_val);
                    return off_val.id;
                }
            }
        }
    
        // 3. Primitive type - load the field value
        let load_val: *Value = self.func.newValue(Op.Load, ir_node.type_idx);
        load_val.addArg(off_val);
        self.assignReg(load_val);
        return load_val.id;
    }

    fn convertFieldValue(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Access field via computed address: left = base_expr, right = offset
        // Following Zig ssa_builder.zig:1108-1130
        let base_val: *Value = self.getOperandValue(ir_node.left);
        let offset: i64 = ir_node.right;
    
        // 1. Add offset to base to get field address
        let off_val: *Value = self.func.newValue(Op.OffPtr, TYPE_I64);
        off_val.addArg(base_val);
        off_val.aux_int = offset;
    
        // Check if result is a struct or array - if so, return address (no load)
        // Both structs and arrays are inline data, not pointers, so we return
        // the address for further field access or indexing.
        // Following Zig ssa_builder.zig:1118-1126
        if self.type_registry != null and ir_node.type_idx > 0 {
            let field_type: *Type = self.type_registry.get(ir_node.type_idx);
            if field_type != null {
                if field_type.kind == TypeKind.Struct or field_type.kind == TypeKind.Array {
                    // Nested struct or array - return address for further access
                    self.assignReg(off_val);
                    return off_val.id;
                }
            }
        }
    
        // 2. Primitive type - load the field value
        let load_val: *Value = self.func.newValue(Op.Load, ir_node.type_idx);
        load_val.addArg(off_val);
        self.assignReg(load_val);
        return load_val.id;
    }

    fn convertStoreFieldLocal(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Store to field of local struct: left = local_idx, right = offset, value = stored_expr
        let loc_idx: i64 = ir_node.left;
        let offset: i64 = ir_node.right;
        let val_val: *Value = self.getOperandValue(ir_node.value);
    
        // 1. Get address of local
        let addr_val: *Value = self.func.newValue(Op.LocalAddr, TYPE_I64);
        addr_val.aux_int = loc_idx;
        self.assignReg(addr_val);
    
        // 2. Add offset to get field address
        let off_val: *Value = self.func.newValue(Op.OffPtr, TYPE_I64);
        off_val.addArg(addr_val);
        off_val.aux_int = offset;
        self.assignReg(off_val);
    
        // 3. Store the value
        let store_val: *Value = self.func.newValue(Op.Store, TYPE_VOID);
        store_val.addArg2(off_val, val_val);
        return store_val.id;
    }

    fn convertStoreField(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Store to field via computed address: left = base_expr, right = offset, value = stored_expr
        let base_val: *Value = self.getOperandValue(ir_node.left);
        let offset: i64 = ir_node.right;
        let val_val: *Value = self.getOperandValue(ir_node.value);
    
        // 1. Add offset to base to get field address
        let off_val: *Value = self.func.newValue(Op.OffPtr, TYPE_I64);
        off_val.addArg(base_val);
        off_val.aux_int = offset;
        self.assignReg(off_val);
    
        // 2. Store the value
        let store_val: *Value = self.func.newValue(Op.Store, TYPE_VOID);
        store_val.addArg2(off_val, val_val);
        return store_val.id;
    }

    fn convertIndexLocal(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Index into local array: left = local_idx, right = index_expr, value = elem_size
        let loc_idx: i64 = ir_node.left;
        let idx_val: *Value = self.getOperandValue(ir_node.right);
        let elem_size: i64 = ir_node.value;
    
        // 1. Get address of local array
        let addr_val: *Value = self.func.newValue(Op.LocalAddr, TYPE_I64);
        addr_val.aux_int = loc_idx;
        self.assignReg(addr_val);
    
        // 2. Compute byte offset: index * elem_size
        let size_val: *Value = self.func.emitConstInt(elem_size, TYPE_I64);
        self.assignReg(size_val);
        let offset_val: *Value = self.func.emitBinary(Op.Mul64, idx_val, size_val, TYPE_I64);
        self.assignReg(offset_val);
    
        // 3. Add offset to base address
        let ptr_val: *Value = self.func.emitBinary(Op.AddPtr, addr_val, offset_val, TYPE_I64);
        self.assignReg(ptr_val);
    
        // 4. Load the element value
        let load_val: *Value = self.func.newValue(Op.Load, ir_node.type_idx);
        load_val.addArg(ptr_val);
        self.assignReg(load_val);
        return load_val.id;
    }

    fn convertIndexValue(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Index via computed address: left = base_expr, right = index_expr, value = elem_size
        // Following Zig ssa_builder.zig:1308-1346
        let base_val: *Value = self.getOperandValue(ir_node.left);
        let idx_val: *Value = self.getOperandValue(ir_node.right);
        let elem_size: i64 = ir_node.value;
    
        // 1. Compute byte offset: index * elem_size
        let size_val: *Value = self.func.emitConstInt(elem_size, TYPE_I64);
        self.assignReg(size_val);
        let offset_val: *Value = self.func.emitBinary(Op.Mul64, idx_val, size_val, TYPE_I64);
        self.assignReg(offset_val);
    
        // 2. Add offset to base address
        let ptr_val: *Value = self.func.emitBinary(Op.AddPtr, base_val, offset_val, TYPE_I64);
        self.assignReg(ptr_val);
    
        // 3. For struct types, return address without loading
        // Following Zig ssa_builder.zig:1333-1337:
        //   const elem_type = self.type_registry.get(node.type_idx);
        //   if (elem_type == .struct_type) { break :blk elem_addr; }
        // Structs need address for field access, not loaded value
        if self.type_registry != null and ir_node.type_idx > 0 {
            let elem_type: *Type = self.type_registry.get(ir_node.type_idx);
            if elem_type.kind == TypeKind.Struct {
                return ptr_val.id;
            }
        }
    
        // 4. Load the element value for primitives
        let load_val: *Value = self.func.newValue(Op.Load, ir_node.type_idx);
        load_val.addArg(ptr_val);
        self.assignReg(load_val);
        return load_val.id;
    }

    fn convertStoreIndexLocal(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Store to local array element: left = local_idx, right = index_expr, value = stored_expr, op = elem_size
        let loc_idx: i64 = ir_node.left;
        let idx_val: *Value = self.getOperandValue(ir_node.right);
        let val_val: *Value = self.getOperandValue(ir_node.value);
        let elem_size: i64 = ir_node.op;
    
        // 1. Get address of local array
        let addr_val: *Value = self.func.newValue(Op.LocalAddr, TYPE_I64);
        addr_val.aux_int = loc_idx;
        self.assignReg(addr_val);
    
        // 2. Compute byte offset: index * elem_size
        let size_val: *Value = self.func.emitConstInt(elem_size, TYPE_I64);
        self.assignReg(size_val);
        let offset_val: *Value = self.func.emitBinary(Op.Mul64, idx_val, size_val, TYPE_I64);
        self.assignReg(offset_val);
    
        // 3. Add offset to base address
        let ptr_val: *Value = self.func.emitBinary(Op.AddPtr, addr_val, offset_val, TYPE_I64);
        self.assignReg(ptr_val);
    
        // 4. Store the value
        // BUG FIX: Propagate elem_size to Store for correct sized store instruction
        // Without this, codegen uses 64-bit STR for byte arrays, corrupting stack
        let store_val: *Value = self.func.newValue(Op.Store, TYPE_VOID);
        store_val.addArg2(ptr_val, val_val);
        store_val.aux_int = elem_size;  // Store element size for codegen
        return store_val.id;
    }

    fn convertStoreIndexValue(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Store via indexed address: left = base_expr, right = index_expr, value = stored_expr, op = elem_size
        let base_val: *Value = self.getOperandValue(ir_node.left);
        let idx_val: *Value = self.getOperandValue(ir_node.right);
        let val_val: *Value = self.getOperandValue(ir_node.value);
        let elem_size: i64 = ir_node.op;
    
        // 1. Compute byte offset: index * elem_size
        let size_val: *Value = self.func.emitConstInt(elem_size, TYPE_I64);
        self.assignReg(size_val);
        let offset_val: *Value = self.func.emitBinary(Op.Mul64, idx_val, size_val, TYPE_I64);
        self.assignReg(offset_val);
    
        // 2. Add offset to base address
        let ptr_val: *Value = self.func.emitBinary(Op.AddPtr, base_val, offset_val, TYPE_I64);
        self.assignReg(ptr_val);
    
        // 3. Store the value
        // BUG FIX: Propagate elem_size to Store for correct sized store instruction
        let store_val: *Value = self.func.newValue(Op.Store, TYPE_VOID);
        store_val.addArg2(ptr_val, val_val);
        store_val.aux_int = elem_size;  // Store element size for codegen
        return store_val.id;
    }

    fn convertMakeString(self: *SSABuilder, ir_node: *IRNode) i64 {
        // @string(ptr, len): left=ptr_expr, right=len_expr (relative IR indices)
        let ptr_val: *Value = self.getOperandValue(ir_node.left);
        let len_val: *Value = self.getOperandValue(ir_node.right);
        let make_val: *Value = self.func.newValue(Op.StringMake, TYPE_STRING);
        make_val.addArg2(ptr_val, len_val);
        self.assignReg(make_val);
        return make_val.id;
    }

    fn convertMakeSlice(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Slice creation: left=ptr_expr, right=len_expr (relative IR indices)
        let ptr_val: *Value = self.getOperandValue(ir_node.left);
        let len_val: *Value = self.getOperandValue(ir_node.right);
        let make_val: *Value = self.func.newValue(Op.SliceMake, TYPE_I64);
        make_val.addArg2(ptr_val, len_val);
        self.assignReg(make_val);
        return make_val.id;
    }

    fn convertSlicePtr(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Extract pointer from slice: left=slice_expr (relative IR index)
        let slice_val: *Value = self.getOperandValue(ir_node.left);
        let ptr_val: *Value = self.func.newValue(Op.SlicePtr, TYPE_I64);
        ptr_val.addArg(slice_val);
        self.assignReg(ptr_val);
        return ptr_val.id;
    }

    fn convertSliceLen(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Extract length from slice: left=slice_expr (relative IR index)
        let slice_val: *Value = self.getOperandValue(ir_node.left);
        let len_val: *Value = self.func.newValue(Op.SliceLen, TYPE_I64);
        len_val.addArg(slice_val);
        self.assignReg(len_val);
        return len_val.id;
    }

    fn convertCall(self: *SSABuilder, ir_node: *IRNode) i64 {
        let val: *Value = self.func.emitCall(
                                         ir_node.func_name_start,
                                         ir_node.func_name_len,
                                         ir_node.type_idx);
        var arg_idx: i64 = 0;
        while arg_idx < ir_node.call_args.count {
            let arg_ir_rel: i64 = i64list_get(&ir_node.call_args, arg_idx);
            // Validate arg index
            if arg_ir_rel < 0 {
                print("SSA ERROR: Invalid arg ir_rel=");
                print(arg_ir_rel);
                print(" for call arg ");
                print(arg_idx);
                print(" in ");
                // Print current function name
                if self.source != null and self.func.name_start >= 0 and self.func.name_len > 0 {
                    write(1, self.source + self.func.name_start, self.func.name_len);
                }
                print(" calling <callee>");
                // Print callee name disabled for now to debug crash
                // if ir_node.func_name_start != 0 and ir_node.func_name_len > 0 {
                //     write(1, @intToPtr(*u8, ir_node.func_name_start), ir_node.func_name_len);
                // }
                print("\n");
                arg_idx = arg_idx + 1;
                continue;
            }
            let abs_idx: i64 = self.ir_nodes_start + arg_ir_rel;
            let arg_ir_node: *IRNode = self.ir_nodes + abs_idx;
            let arg_value_id: i64 = self.convertNode(arg_ir_node, abs_idx);
            let arg_val: *Value = self.func.getValue(arg_value_id);
            val.addArg(arg_val);
            arg_idx = arg_idx + 1;
        }
        self.assignReg(val);
        return val.id;
    }

    fn convertCallIndirect(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Create ClosureCall value
        let val: *Value = self.func.emitCallIndirect(TYPE_I64);
    
        // First argument is the function pointer (ir_node.left)
        // Use convertNode pattern like Zig
        let fn_ptr_abs_idx: i64 = self.ir_nodes_start + ir_node.left;
        let fn_ptr_ir_node: *IRNode = self.ir_nodes + fn_ptr_abs_idx;
        let fn_ptr_value_id: i64 = self.convertNode(fn_ptr_ir_node, fn_ptr_abs_idx);
        let fn_ptr_val: *Value = self.func.getValue(fn_ptr_value_id);
        val.addArg(fn_ptr_val);
    
        // Add remaining arguments - call convertNode for each arg (matches Zig)
        var arg_idx: i64 = 0;
        while arg_idx < ir_node.call_args.count {
            let arg_ir_rel: i64 = i64list_get(&ir_node.call_args, arg_idx);
            let abs_idx: i64 = self.ir_nodes_start + arg_ir_rel;
            let arg_ir_node: *IRNode = self.ir_nodes + abs_idx;
            let arg_value_id: i64 = self.convertNode(arg_ir_node, abs_idx);
            let arg_val: *Value = self.func.getValue(arg_value_id);
            val.addArg(arg_val);
            arg_idx = arg_idx + 1;
        }
        self.assignReg(val);
        return val.id;
    }

    fn convertStrConcat(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Create StringConcat value (returns string type)
        let val: *Value = self.func.newValue(Op.StringConcat, TYPE_STRING);
    
        // Convert and add the 4 args: ptr1, len1, ptr2, len2
        var arg_idx: i64 = 0;
        while arg_idx < ir_node.call_args.count {
            let arg_ir_rel: i64 = i64list_get(&ir_node.call_args, arg_idx);
            if arg_ir_rel < 0 {
                arg_idx = arg_idx + 1;
                continue;
            }
            let abs_idx: i64 = self.ir_nodes_start + arg_ir_rel;
            let arg_ir_node: *IRNode = self.ir_nodes + abs_idx;
            let arg_value_id: i64 = self.convertNode(arg_ir_node, abs_idx);
            let arg_val: *Value = self.func.getValue(arg_value_id);
            val.addArg(arg_val);
            arg_idx = arg_idx + 1;
        }
    
        self.assignReg(val);
        return val.id;
    }

    fn convertSelect(self: *SSABuilder, ir_node: *IRNode) i64 {
        // Select: left = cond, right = true_val, value = false_val
        let cond_val: *Value = self.getOperandValue(ir_node.left);
        let true_val: *Value = self.getOperandValue(ir_node.right);
        let false_val: *Value = self.getOperandValue(ir_node.value);
        let val: *Value = self.func.emitSelect(cond_val, true_val, false_val, TYPE_I64);
        self.assignReg(val);
        return val.id;
    }

    fn convertBranch(self: *SSABuilder, ir_node: *IRNode) {
        // left is relative IR index of condition
        let cond_val: *Value = self.getOperandValue(ir_node.left);
        let then_block: i64 = ir_node.right;
        let else_block: i64 = ir_node.value;
        self.func.emitIf(cond_val, then_block, else_block);
    }

    fn convertJump(self: *SSABuilder, ir_node: *IRNode) {
        let target_block: i64 = ir_node.left;
        self.func.emitJump(target_block);
    }

    fn convertReturn(self: *SSABuilder, ir_node: *IRNode) {
        if ir_node.left >= 0 {
            let ret_val: *Value = self.getOperandValue(ir_node.left);
            self.func.emitReturn(ret_val);
        } else {
            // Void return - emit return with no value
            let void_ret: *Value = self.func.newValue(Op.Return, TYPE_VOID);
        }
    }

    fn getBlockDefs(self: *SSABuilder, block_id: i64) *BlockDefs {
        // Bounds check
        if block_id < 0 or block_id >= self.all_defs_cap {
            return null;
        }
        let bd: *BlockDefs = self.all_defs + block_id;
        if bd.initialized {
            return bd;
        }
        return null;
    }

    fn copyDefs(self: *SSABuilder, from_block: i64, to_block: i64) {
        let from: *BlockDefs = self.getBlockDefs(from_block);
        let to: *BlockDefs = self.getBlockDefs(to_block);
    
        if from == null or to == null { return; }
    
        // Copy all defined variables (iterate through all indices)
        var var_idx: i64 = 0;
        while var_idx < from.values_cap {
            let value_id: i64 = (from.values + var_idx).*;
            if value_id != INVALID_ID {
                BlockDefs_set(to, var_idx, value_id);
            }
            var_idx = var_idx + 1;
        }
    }

    fn verify(self: *SSABuilder) bool {
        let f: *Func = self.func;
        var errors_found: bool = false;
    
        var block_idx: i64 = 0;
        while block_idx < f.blocks_count {
            let block: *Block = f.blocks + block_idx;
    
            // Check values in this block
            var seen_non_phi: bool = false;
            var val_idx: i64 = block.values_start;
            while val_idx < block.values_start + block.values_count {
                let v: *Value = f.values + val_idx;
    
                // Check 1: Phi placement
                if v.op == Op.Phi {
                    if seen_non_phi {
                        // Error: phi after non-phi value
                        errors_found = true;
                    }
                    // Check 2: Phi arg count matches predecessors
                    if v.args.count != block.preds.count {
                        errors_found = true;
                    }
                } else {
                    seen_non_phi = true;
                }
    
                // Check 3: No unresolved FwdRefs
                if v.op == Op.FwdRef {
                    errors_found = true;
                }
    
                val_idx = val_idx + 1;
            }
    
            // Check 4: Block termination
            if block.kind == BlockKind.Return {
                // Return blocks should have no successors
                if block.succs_count != 0 {
                    errors_found = true;
                }
            } else if block.kind == BlockKind.If {
                // If blocks must have exactly 2 successors
                if block.succs_count != 2 {
                    errors_found = true;
                }
                // Must have a control value (condition)
                if block.control < 0 {
                    errors_found = true;
                }
            }
            // Plain blocks: allow 0 or 1 successors (single-block functions, etc.)
    
            block_idx = block_idx + 1;
        }
    
        return not errors_found;
    }

}
